{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math for Data Scientists: Matrices and Gradient Descent\n",
    "\n",
    "Matrices are a fundamental aspect of data science models and problems, including image processing, deep learning, NLP, and PCA. You will encounter matrices *many* times in your career as a data scientist. Matrices are a fundamental tool in **linear algebra**.\n",
    "\n",
    "Gradient Descent is an optimization procedure that, in one form or another, underlies many model-fitting algorithms. The gradient, in concept, is a generalized notion of a derivative and so belongs to **calculus** or **analysis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beginning with Plain Old Algebra\n",
    "\n",
    "Linear algebra can be used to solve for multiple unknowns at once. Let's start with a one-variable \"system\" before moving on to two-, three-, or many-variable systems.\n",
    "\n",
    "Suppose we start with a one-variable system like $2X = 10$.\n",
    "\n",
    "How do we solve this?\n",
    "\n",
    "Now consider a two-variable system:\n",
    "\n",
    "$2X + 4Y = 10 \\\\\n",
    "X + 4Y = 7$\n",
    "\n",
    "### Solution through Substitution\n",
    "We _could_ solve this system by taking the first equation, solving it for X, and then plugging the result into the second:\n",
    "\n",
    "$2X + 4Y = 10$. <br/> Thus: $\\\\ 2X = 10 - 4Y \\\\ X = 5 - 2Y$.\n",
    "\n",
    "Plugging in to the second equation, we have:\n",
    "\n",
    "$5 - 2Y + 4Y = 7$. <br/> Thus: $\\\\ 5 + 2Y = 7 \\\\ 2Y = 2 \\\\ Y = 1$.\n",
    "\n",
    "Plugging this back into the first equation, we have:\n",
    "\n",
    "$2X + 4 = 10$.  <br/> Thus: $\\\\ 2X = 6 \\\\ X = 3$.\n",
    "\n",
    "And we have our solutions:  $X = 3, Y = 1$.\n",
    "\n",
    "But this is computationally _very slow_! There is a better way:\n",
    "\n",
    "### Solution through Elimination\n",
    "\n",
    "Much faster is to subtract the second equation from the first:\n",
    "\n",
    "If $2X + 4Y = 10$ and $X + 4Y = 7$,\n",
    "then $(2X - X) + (4Y - 4Y) = 10 - 7$, i.e. $X = 3$. Then I could subtract this ($X + 0Y = 3$) from $X + 4Y = 7$, yielding: $4Y = 4$, i.e. $Y = 1$.\n",
    "\n",
    "We can represent this in matrix form using the equations as our rows. The columns will correspond to the variables:\n",
    "\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "2 & 4 & 10 \\\\\n",
    "1 & 4 & 7\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\rightarrow \\begin{bmatrix}\n",
    "1 & 0 & 3 \\\\\n",
    "1 & 4 & 7\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\rightarrow \\begin{bmatrix}\n",
    "1 & 0 & 3 \\\\\n",
    "0 & 4 & 4\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\rightarrow \\begin{bmatrix}\n",
    "1 & 0 & 3 \\\\\n",
    "0 & 1 & 1\n",
    "\\end{bmatrix}$\n",
    "\n",
    "This is the matrix way of saying that X = 3 and that Y = 1.\n",
    "\n",
    "There are lots of strategies in linear algebra for \"reducing\" a matrix to a form where there are ones down the main diagonal and zeroes everywhere else (except the rightmost column), because such a matrix represents a list of \"already solved\" equations: <br/>\n",
    "\n",
    "$\\begin{bmatrix}\n",
    "1 & 0 & 0 & ... & 0 & b_1 \\\\\n",
    "0 & 1 & 0 & ... & 0 & b_2 \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "... & ... & ... & ... & ... & ... \\\\\n",
    "0 & 0 & 0 & ... & 1 & b_n\n",
    "\\end{bmatrix}$\n",
    "\n",
    "$\\; \\downarrow$\n",
    "\n",
    "$X_1 + 0X_2 + 0X_3 + ... + 0X_n = b_1 \\\\\n",
    "0X_1 + X_2 + 0X_3 + ... + 0X_n = b_2 \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "0X_1 + 0X_2 + ... + 0X_{n-1} + X_n = b_n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scalars to Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A _scalar_ has simply a single value. Any real number can be the value of a scalar.\n",
    "\n",
    "A _vector_ must be specified by _two_ parameters: magnitude and direction. In a Cartesian coordinate system, a vector $\\vec{v}$ will generally be specified by its x- and y-components, $v_x$ and $v_y$.\n",
    "\n",
    "In that case: <br/>\n",
    "    \\- The magnitude of $v$ is given by $||v|| = \\sqrt{v^2_x + v^2_y}$ <br/>\n",
    "    \\- The direction of $v$ is given by $\\theta = tan^{-1}\\left(\\frac{v_y}{v_x}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Video on [vectors](https://www.youtube.com/watch?v=fNk_zzaMoSs&list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Arithmetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Addition\n",
    "\n",
    "Vector addition is simple: Just add the x- and the y-components together:\n",
    "\n",
    "$(8, 14) + (7, 6) = (15, 20)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider the vectors (8, 14) and (7, 6). Let's try using Python\n",
    "# to add them together.\n",
    "\n",
    "vec_1 = (8, 14)\n",
    "vec_2 = (7, 6)\n",
    "\n",
    "vec_1 + vec_2 == (15, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_1 + vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try typing 'vec_1.' and then pressing TAB. What options do we have here?\n",
    "\n",
    "vec_1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base Python is not particularly good for non-scalar arithmetic. This is one of many places where NumPy can come in very handy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try this again, but this time we'll use NumPy arrays:\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "vec_1, vec_2 = np.array([8, 14]), np.array([7, 6])\n",
    "vec_1 + vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is base Python any better for vector _multiplication_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What happens if we multiply 3 by the vector below?\n",
    "\n",
    "vec_1 = (4, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 * vec_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try multiplying the vectors (4, 14) and (8, 6)):\n",
    "\n",
    "vec_2 = (8, 6)\n",
    "\n",
    "vec_1 * vec_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened? Why did we get an error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact there are multiple ways of understanding the notion of vector multiplication. All are potentially useful, but the one that we'll likely be of most use is the *dot-product*, which is defined as follows:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a & b \\\\\n",
    "\\end{bmatrix}\n",
    ". \n",
    "\\begin{bmatrix}\n",
    "c \\\\\n",
    "d\n",
    "\\end{bmatrix}\n",
    "=\n",
    "ac + bd\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot-product is the sum of the pariwise products of the vectors' entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've got the vectors stored as NumPy arrays, let's once again\n",
    "# try typing 'vec_1.' and then pressing TAB.\n",
    "\n",
    "vec_1.\n",
    "\n",
    "# Now we have many options! Notice that one of these options is 'dot'.\n",
    "# This is our dot-product!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the .dot() method to calculate the dot-product of our two vectors:\n",
    "\n",
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Higher Dimensions: From Vectors to Matrices\n",
    "\n",
    "For higher dimensions we can use _matrices_ to express ourselves. Suppose we had a two-variable system:\n",
    "\n",
    "\\begin{align}\n",
    "a_{1,1}x_1 + a_{1,2}x_2 = c_1 \\\\\n",
    "a_{2,1}x_1 + a_{2,2}x_2 = c_2\n",
    "\\end{align}\n",
    "\n",
    "We can write this as:\n",
    "\n",
    "$A\\vec{x} = \\vec{c}$,\n",
    "\n",
    "where now $\\vec{x}$ is the _vector_ $(x_1, x_2)$ and $\\vec{c}$ is the _vector_ $(c_1, c_2)$.\n",
    "\n",
    "Similarly, $A$ is the _matrix_ of coefficients that describe our system:\n",
    "\\begin{equation} A = \n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "c_1 \\\\\n",
    "c_2\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Ways to Multiply\n",
    "\n",
    "Just as there were different notions of \"multiplication\" for vectors, so too there are different notions of multiplication for matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dot-Product\n",
    "Very often when people talk about multiplying matrices they'll mean the dot-product:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "a_{1,1} & a_{1,2} \\\\\n",
    "a_{2,1} & a_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\times\n",
    "\\begin{bmatrix}\n",
    "b_{1,1} & b_{1,2} \\\\\n",
    "b_{2,1} & b_{2,2}\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "a_{1,1}\\times b_{1,1} + a_{1,2}\\times b_{2,1} & a_{1,1}\\times b_{1,2} + a_{1,2}\\times b_{2,2} \\\\\n",
    "a_{2,1}\\times b_{1,1} + a_{2,2}\\times b_{2,1} & a_{2,1}\\times b_{1,2} + a_{2,2}\\times b_{2,2}\n",
    "\\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "Take the entries in each *row* of the left matrix and multiply them, respectively, by the entries in each *column* of the right matrix, and then add them up. This is the product we calculated above with our two vectors!\n",
    "\n",
    "Note that matrix dot-multiplication is NOT commutative! In general, $AB \\neq BA$.\n",
    "\n",
    "#### A note about vectors and matrices\n",
    "\n",
    "Strictly speaking, this is true for vectors as well. Above, we multiplied the *row*-vector $(a, b)$ by the *column*-vector $(c, d)$. A row-vector is simply a matrix with only one row; a column-vector is simply a matrix with only one column. What would be the result of multiplying the column-vector $(c, d)$ on the left by the row-vector $(a, b)$ on the right?\n",
    "\n",
    "<details><summary>\n",
    "    Ans.:\n",
    "        </summary>\n",
    "    \\begin{equation}\n",
    "    \\begin{bmatrix}\n",
    "    c \\\\\n",
    "    d\n",
    "    \\end{bmatrix}\n",
    "    \\times\n",
    "    \\begin{bmatrix}\n",
    "    a & b\n",
    "    \\end{bmatrix}\n",
    "    =\n",
    "    \\begin{bmatrix}\n",
    "    ca & cb \\\\\n",
    "    da & db\n",
    "    \\end{bmatrix}\n",
    "    \\end{equation}\n",
    "    </details>\n",
    "\n",
    "#### Features of the Matrix Dot-Product\n",
    "\n",
    "Observe also that in order to be able to perform the dot product on two matrices A and B, the number of columns of A must equal the number of rows of B.\n",
    "\n",
    "Also, the number of rows of the _product_ matrix will equal the number of rows of A, and the number of columns of the product matrix will equal the number of columns of B.\n",
    "\n",
    "In order to solve an equation like $A\\vec{x} = \\vec{c}$ for $\\vec{x}$, we can't very well divide $\\vec{c}$ by $A$! But there is a notion of matrix _inversion_ that is relevant here, which is analogous to multiplicative inversion. If we have an equation like $2x = 10$, we can simply multiply both sides by the multiplicative inverse of the coefficient of $x$, viz. $2^{-1}$. And here the point, of course, is that $2^{-1} \\times 2 = 1$.\n",
    "\n",
    "In the higher-dimensional case, what we can do is to left-multiply both sides by the _inverse matrix_ of A, denoted $A^{-1}$, and here the point is that the dot-product $A^{-1}A = I$, where $I$ is the identity matrix containing 1's along the main diagonal (upper-left to lower-right) and 0's everywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using NumPy arrays, dot-multiply the matrices\n",
    "\\begin{bmatrix}\n",
    "3 & 2 \\\\\n",
    "5 & 7\n",
    "\\end{bmatrix}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{bmatrix}\n",
    "2 & 4 \\\\\n",
    "3 & 10\n",
    "\\end{bmatrix}\n",
    "\n",
    "in the code-cell below. Remember that you need square brackets around the whole array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "Sometimes you will encounter _tensors_ in your work. A tensor is to a matrix as a matrix is to a vector. A vector has one representational dimension and a matrix has two. If you need an object with three or more representational dimensions, you're talking about a tensor. A tensor has rows (that run from left to right), columns (that run from top to bottom), and _tubes_ (that run from front to back)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Typical Data Science Problems\n",
    "\n",
    "Consider a typical dataset and the associated multiple linear regression problem. We have many observations (rows), each of which consists of a set of values both for the predictors (columns, i.e. the independent variables) and for the target (the dependent variable).\n",
    "\n",
    "We can think of the values of the independent variables as our matrix $A$ of coefficients and of the values of the dependent variable as our output vector $\\vec{c}$.\n",
    "\n",
    "The task here is, in effect, to solve for $\\vec{\\beta}$, where we have that $A\\vec{\\beta} = \\vec{c}$, except in general we'll have more rows than columns. This is why we won't in general be computing matrix inverses. (They're computationally expensive, anyway.) This is also why we have a problem requiring not a direct solution but rather an optimization--in our case, a best-fit line.\n",
    "\n",
    "Using $z$ for our independent variables and $y$ for our dependent variable, we have:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\beta_1\\begin{bmatrix}\n",
    "z_{1,1} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{m,1}\n",
    "\\end{bmatrix} +\n",
    "... + \\beta_n\\begin{bmatrix}\n",
    "z_{1,n} \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "z_{m,n}\n",
    "\\end{bmatrix} = \\begin{bmatrix}\n",
    "y_1 \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    ".  \\\\\n",
    "y_m\n",
    "\\end{bmatrix}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using NumPy to Solve a System of Linear Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy's ```linalg``` module has a ```.solve()``` method that you can use to solve a system of linear equations!\n",
    "\n",
    "In particular, it will solve for the vector $\\vec{x}$ in the equation $A\\vec{x} = b$. You should know that, \"under the hood\", the ```.solve()``` method does NOT compute the inverse matrix $A^{-1}$. Check out [this discussion](https://stackoverflow.com/questions/31256252/why-does-numpy-linalg-solve-offer-more-precise-matrix-inversions-than-numpy-li) on stackoverflow for a helpful discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check out the documentation for ```.solve()``` [here](https://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the .solve() method to solve this system of equations\n",
    "\n",
    "X = np.array([[2, -1, 4, 6, 3], [4, 7, 1, 1, 12],\n",
    "             [9, 14, 2, 2, 6], [1, 1, 1, 2, 17],\n",
    "             [-3, -2, -6, 12, -5]])\n",
    "\n",
    "y = np.array([3, 15, 20, 2, -6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.solve(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Algebra Solves the Best-Fit Line Problem\n",
    "\n",
    "If we have a matrix of predictors $X$ and a target column $y$, we can express $\\hat{y}$, the best-fit line, as  follows:\n",
    "\n",
    "$\\large\\hat{y} = (X^TX)^{-1}X^Ty$.\n",
    "\n",
    "$(X^TX)^{-1}X^T$ is sometimes called the *pseudo-inverse* of $X$. We'll have more to say about this in a future lesson when we talk about the singular value decomposition.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(list(zip(np.random.normal(size=10), np.array(np.random.normal(size=10, loc=2)))))\n",
    "target = np.array(np.random.exponential(size=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.inv(preds.T.dot(preds)).dot(preds.T).dot(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LinearRegression(fit_intercept=False).fit(preds, target).coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculus\n",
    "\n",
    "But sometimes this computation is too expensive. And for other sorts of loss functions, it may not be available at all. So we turn to numerical methods, and, in particular, to the method of gradient descent. In order to gain a good grasp of this method, we'll have to review a couple items from calculus, namely:\n",
    "\n",
    "- the Chain Rule; and\n",
    "- partial differentiation.\n",
    "\n",
    "### The Chain Rule\n",
    "\n",
    "$\\large\\frac{d}{dx}[f(g(x))] = f'(g(x))g'(x)$\n",
    "\n",
    "That is: The derivative of a *composition* of functions is: the derivative of the first applied to the second, multiplied by the derivative of the second.\n",
    "\n",
    "So if we know e.g. that $\\frac{d}{dx}[e^x] = e^x$ and $\\frac{d}{dx}[x^2] = 2x$, then we can use the Chain Rule to calculate $\\frac{d}{dx}[e^{x^2}]$. We set $f(x) = e^x$ and $g(x) = x^2$, so the derivative must be:\n",
    "\n",
    "$\\large\\frac{d}{dx}[e^{x^2}] = (e^{x^2})(2x) = 2xe^{x^2}$.\n",
    "\n",
    "### Partial Differentiation\n",
    "\n",
    "Partial differentiation is required for functions of multiple variables. If e.g. I have some function $h = h(a, b)$, then I can consider how $h$ changes with respect to $a$ (while keeping $b$ constant)––that's $\\frac{\\partial h}{\\partial a}$, and I can consider how $h$ changes with respect to $b$ (while keeping $a$ constant)––that's $\\frac{\\partial h}{\\partial b}$. And so the rule is simple enough: If I'm differentiating my function with respect to some variable, I'll **treat all other variables as constants**.\n",
    "\n",
    "So e.g. if \n",
    "\n",
    "Suppose our loss function looks like this:\n",
    "\n",
    "$\\large\\xi(x, y, z) = x^2y^5z^3 - ze^{cos(xy)} + (yz)^3$;\n",
    "\n",
    "for some parameters $x$, $y$, and $z$.\n",
    "\n",
    "What are the partial derivatives of this function?\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial x} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $2xy^5z^3 + yze^{cos(xy)}sin(xy)$\n",
    "    </details>\n",
    "<br/>\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial y} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $5x^2y^4z^3 + xze^{cos(xy)}sin(xy) + 3y^2z^3$\n",
    "    </details>\n",
    "<br/>\n",
    "\n",
    "$\\large\\frac{\\partial\\xi}{\\partial z} = ?$\n",
    "\n",
    "<br/>\n",
    "<details>\n",
    "    <summary>\n",
    "        Check\n",
    "    </summary>\n",
    "    <br/>\n",
    "    $3x^2y^5z^2 - e^{cos(xy)} + 3y^3z^2$\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Suppose that we have these data points:\n",
    "\n",
    "(1, 2), (3, 9), (5, 10),\n",
    "\n",
    "and that we're interested in constructing a best-fit line through them.\n",
    "\n",
    "Now this optimization problem has a well-known solution and we could simply plug in our values here into that formula.\n",
    "\n",
    "The solution to a linear regression problem is a matter of minimizing the sum of squared distances between actual y-values and the line's predictions.\n",
    "\n",
    "More generally, model optimization involves setting the derivative of the function you want to minimize (for a linear regression, this would be the sum of the squared distances between $y$ and $\\hat{y}$) to zero, and then solving for the parameters that define the model (for a linear regression, this would be the slope and y-intercept).\n",
    "\n",
    "But this is not always easily or efficiently done. Sometimes the loss function is quite complicated and it is not a straightforward matter to set the derivative equal to zero and to solve the resulting equation(s) for the missing parameters.\n",
    "\n",
    "Sometimes it's easier or more efficient to:\n",
    "\n",
    "### Gradient Descent in Words\n",
    "- make a guess at where the function attains its minimum value; and then\n",
    "- calculate the derivative at that point; and then\n",
    "- use that value to decide how to make your next guess!\n",
    "\n",
    "Repeat until we get the derivative as close as we like to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This procedure (in particular, the last step) is possible because of how the gradient is defined.\n",
    "\n",
    "The gradient is a kind of higher-order derivative for multi-variable functions. In particular, it is a vector defined as follows:\n",
    "\n",
    "$\\large\\nabla f(x_1, ... , x_n) = \\frac{\\partial f}{\\partial x_1}\\hat{x_1} + ... + \\frac{\\partial f}{\\partial x_n}\\hat{x_n}$\n",
    "\n",
    "where $\\hat{x_k}$ is the unit vector in the $\\vec{x_k}$-direction.\n",
    " \n",
    "Recall that, in the single-variable case, a positive derivative indicates an increasing function and a negative derivative indicates a decreasing function.\n",
    "\n",
    "In the multivariate case, the gradient tells us how the function is changing **in each dimension**. And that means that **the gradient will point in the direction of steepest increase**.\n",
    "\n",
    "Let's look back at our [3-d plotter](https://academo.org/demos/3d-surface-plotter/).\n",
    "\n",
    "Therefore, if we want to improve our guess at the minimum of our loss function, we'll move in the **opposite direction** of the gradient away from our last guess. Hence we are using the *gradient* of our loss function to *descend* to the minimum value of that loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Let's try this with our three data points above.\n",
    "\n",
    "You may recall that the parameters that produce the best-fit line for the three points above are:\n",
    "\n",
    "$\\beta_0 = 1$; <br/>\n",
    "$\\beta_1 = 2$.\n",
    "\n",
    "But suppose we try to arrive at these values by guessing and checking, i.e. by the method of gradient descent.\n",
    "\n",
    "Let's start with a guess of:\n",
    "\n",
    "$\\beta_0 = 3$; <br/>\n",
    "$\\beta_1 = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_0 = 3\n",
    "beta_1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the function we're trying to minimize is:\n",
    "\n",
    "$\\large f(\\beta_0, \\beta_1) = \\Sigma(y - \\beta_1x - \\beta_0)^2$.\n",
    "\n",
    "So we have:\n",
    "\n",
    "$\\large\\frac{\\partial f}{\\partial\\beta_0} = -2\\Sigma(y - \\beta_1x - \\beta_0)$; and\n",
    "\n",
    "$\\large\\frac{\\partial f}{\\partial\\beta_1} = -2\\Sigma x(y - \\beta_1x - \\beta_0)$.\n",
    "\n",
    "Note that if we plug in $\\beta_0$ = 1, $\\beta_1$ = 2, we get 0 for these derivatives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfdbeta0(beta_0, beta_1):\n",
    "    return -2 * ((2 - beta_1*1 - beta_0) + (9 - beta_1*3 - beta_0) + (10 - beta_1*5 - beta_0))\n",
    "\n",
    "def dfdbeta1(beta_0, beta_1):\n",
    "    return -2 * ((1 * (2 - beta_1*1 - beta_0)) + (3 * (9 - beta_1*3 - beta_0)) + (5 * (10 - beta_1*5 - beta_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(1, 2)\n",
    "partial_1 = dfdbeta1(1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plug in our guesses and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(3, 3)\n",
    "partial_1 = dfdbeta1(3, 3)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $\\frac{\\partial f}{\\partial\\beta_0}$ and $\\frac{\\partial f}{\\partial\\beta_1}$ are positive, this tells us that we need to make our guesses **smaller** for each. So we might try $\\beta_0 = \\beta_1 = 1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_0 = dfdbeta0(1, 1)\n",
    "partial_1 = dfdbeta1(1, 1)\n",
    "\n",
    "partial_0, partial_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to make our guesses **larger**! Note that, even though $\\beta_0 = 1$ is part of the optimal solution, there is no guarantee that the associated partial derivative will be 0 for all the points in parameter space where $\\beta_0 = 1$, since the derivative is a function both of $\\beta_0$ and of $\\beta_1$.\n",
    "\n",
    "## Learning Rate\n",
    "\n",
    "How can we guarantee that we don't step **too far** when making a new estimate of the optimal parameter values?\n",
    "\n",
    "Here's an elegant solution: Make the size of your step proportional to the value of the derivative at the point where you currently are in parameter space! If we're very far from the minimum, then our values will be large, and so we therefore can safely take a large step; if we're close to the minimum, then our values will be small, and so we should therefore take a smaller step.\n",
    "\n",
    "The size of the step is often called the \"learning rate\". Note that there is reason also not to have your step size be **too small**. If you're far from the minimum, then a small learning rate would make your route to the minimum very costly (in terms of computation time, if nothing else)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula\n",
    "\n",
    "The general algorithm looks like this:\n",
    "\n",
    "We'll make a guess, $\\vec{s}$, at where our loss function attains a minimum. If we're not happy with how close the value of the gradient there is to 0, then we'll make a new guess, and the new guess will be constructed as follows:\n",
    "\n",
    "$\\large\\vec{s}_{new} = \\vec{s}_{old} - \\alpha\\nabla f(\\vec{s}_{old})$,\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "In the one-dimensional case, we'll have:\n",
    "\n",
    "$\\large x_{new} = x_{old} - \\alpha\\frac{d}{dx}|_{x_{old}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Let's write a function that will utilize gradient descent for a simple one-parameter loss function.\n",
    "\n",
    "The inputs will be:\n",
    "\n",
    "- a function representing the derivative of our loss function;\n",
    "- an initial guess;\n",
    "- a learning rate;\n",
    "- a level of desired precision\n",
    "- a maximum number of iterations to run through before quitting\n",
    "\n",
    "I've started the code for you below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_d_grad_desc(deriv, guess, alpha=0.1, prec=0.000001, max_iter=10**4):\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test It!\n",
    "\n",
    "Once you've got the function coded, try it out on some simple one-dimensional functions!\n",
    "\n",
    "Try it on:\n",
    "\n",
    "- `deriv` = $2x$. What answer should you get here?\n",
    "\n",
    "- `deriv` = $10^x - 100$. What answer should you get here?\n",
    "\n",
    "- **loss function** = $3x^3 - 3x^2 + x - 2$\n",
    "\n",
    "- **loss function** = $(4x + 5)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
